{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cognite.client import CogniteClient\n",
    "\n",
    "from cognite.client.data_classes.three_d import ThreeDAssetMapping\n",
    "\n",
    "from cognite.datastudio.entity_matcher import EntityMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we provide functions to simplify the logics of the notebook\n",
    "from utils import chunk_create_rules_df, chunk_predict, get_matches_with_rules\n",
    "\n",
    "# we provide functions to install 3d-nodes, assets and asset-mappings\n",
    "from data_load_cdf import load_assets, load_threednodes, filter_df_threednodes, load_asset_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"my_project\"\n",
    "api_key_name = \"COGNITE_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from getpass import getpass\n",
    "api_key = getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CogniteClient(api_key, project, \"local-jupyter-notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 3d model_id and revision\n",
    "model_id = 1078941578276888\n",
    "revision_id = 506407845865623\n",
    "\n",
    "# define root_id for assets\n",
    "root_id = 8129784932439587\n",
    "\n",
    "#define a name to store thre predicted result\n",
    "entity_matcher_results_file = \"enma_skarv_fpso.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 3d nodes, it might take time\n",
    "df_threednodes = load_threednodes(client, model_id, revision_id)\n",
    "# filter the names of the 3d nodes that do not need contexualization\n",
    "df_threednodes = filter_df_threednodes(df_threednodes, key_words=(\"EQUIPMENT\", \"BRANCH\", \"STRUCTURE\", \" of \"))\n",
    "df_threednodes.rename(columns={\"name\": \"left_side_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download assets\n",
    "df_assets = load_assets(client, root_id).rename(columns={\"name\": \"right_side_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download existing asset mappings from the 3d model\n",
    "df_asset_mappings = load_asset_mappings(client, model_id, revision_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since df_asset_mappings includes only IDs,\n",
    "# in order to get the names we join on the \n",
    "# df_assets, df_threednodes including available respective IDs and names.\n",
    "df_existing_matches = (\n",
    "        df_asset_mappings[[\"nodeId\", \"assetId\"]]\n",
    "        .merge(\n",
    "            df_assets[[\"id\", \"right_side_name\"]],\n",
    "            how=\"left\",\n",
    "            left_on=\"assetId\",\n",
    "            right_on=\"id\",\n",
    "        )\n",
    "        .drop(columns=\"id\")\n",
    "        .merge(\n",
    "            df_threednodes[[\"id\", \"left_side_name\"]],\n",
    "            how=\"left\",\n",
    "            left_on=\"nodeId\",\n",
    "            right_on=\"id\",\n",
    "        )[[\"left_side_name\", \"right_side_name\"]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Matching with rules steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the entity matcher\n",
    "entity_matcher = EntityMatcher(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rules form the existing mappings if exist\n",
    "df_matches = \\\n",
    "    df_existing_matches[[\"left_side_name\", \"right_side_name\"]]\\\n",
    "    .dropna()\\\n",
    "    .rename(columns = {\"left_side_name\": \"input\", \"right_side_name\": \"predicted\"})\n",
    "df_matches[\"score\"] = 1.0\n",
    "\n",
    "pd_rules_from_existing = chunk_create_rules_df(entity_matcher, df_matches.to_dict('records'), size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions \n",
    "if os.path.exists(entity_matcher_results_file):\n",
    "    print(\"Loading predicted from local...\")\n",
    "    with open(entity_matcher_results_file, \"r\") as f:\n",
    "        predicted_matches = json.load(f)\n",
    "else:\n",
    "    model = entity_matcher.fit(df_assets[\"right_side_name\"].tolist())\n",
    "    predicted_matches = chunk_predict(model, df_threednodes[\"left_side_name\"].tolist(), 100000)\n",
    "    # store all predictions in a file\n",
    "    with open(entity_matcher_results_file, \"w\") as f:\n",
    "        json.dump(predicted_matches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We filter predicted_matches on NA-s\n",
    "# We also need to reset index to match order before creating rules\n",
    "df_predicted_matches = pd.DataFrame.from_dict(predicted_matches).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create rules for predicted matches\n",
    "pd_rules_from_predicted = chunk_create_rules_df(entity_matcher, df_predicted_matches.to_dict('records'), size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Associate matches with rules\n",
    "df_predicted_with_rules = get_matches_with_rules(df_predicted_matches, pd_rules_from_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assosicate predicted results with IDs\n",
    "df_predicted_results_raw = df_predicted_with_rules\\\n",
    "    .merge(df_assets, left_on=\"predicted\", right_on=\"right_side_name\", how=\"inner\")\\\n",
    "    .drop(columns=[\"right_side_name\"])\\\n",
    "    .rename(columns={\"id\":\"asset_id\"})\\\n",
    "    .merge(df_threednodes, left_on=\"input\", right_on=\"left_side_name\", how=\"inner\")\\\n",
    "    .drop(columns=[\"left_side_name\"])\\\n",
    "    .rename(columns={\"id\":\"node_id\"})\n",
    "df_predicted_results_raw.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify the filtering of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_predicted_results = df_predicted_results_raw.copy()\n",
    "\n",
    "# comment or uncomment for different filtering\n",
    "\n",
    "# ---\n",
    "# filter on the score value \n",
    "df_predicted_results = df_predicted_results[df_predicted_results[\"score\"] > 0.0]\n",
    "\n",
    "# ---\n",
    "# filter on the avgScore value\n",
    "df_predicted_results = df_predicted_results[df_predicted_results[\"avgScore\"] > 0.0]\n",
    "\n",
    "# ---\n",
    "# filter by the number of matcher per rule\n",
    "df_predicted_results = df_predicted_results[df_predicted_results[\"numMatches\"] > 0]\n",
    "\n",
    "# ---\n",
    "# filter by merging on existing rules only\n",
    "df_predicted_results = df_predicted_results.merge(pd_rules_from_existing\\\n",
    "    .rename(columns={\"numMatches\": \"numMatchesExisting\"})\n",
    "    .drop(columns=[\"avgScore\",\"matchIndex\"]), on=[\"inputPattern\", \"predictPattern\"],\n",
    "    how=\"inner\")\n",
    "\n",
    "# ---\n",
    "# filter out the input 3d-nodes with existing asset mappings \n",
    "# or the predicted assets associated already to a 3d node via an asset mappings\n",
    "df_predicted_results = df_predicted_results\\\n",
    "    .merge(df_existing_matches.rename(columns={\"right_side_name\": \"existing_matching_asset\"}), \n",
    "           left_on=[\"input\"], \n",
    "           right_on=[\"left_side_name\"], \n",
    "           how=\"left\")\\\n",
    "    .drop(columns=[\"left_side_name\"])\n",
    "\n",
    "# filter out matching to the assets associated already to a 3d node via an asset mappings\n",
    "df_predicted_results = df_predicted_results\\\n",
    "    .merge(df_existing_matches.rename(columns={\"left_side_name\": \"existing_matching_3dnode\"}), \n",
    "           left_on=[\"predicted\"], \n",
    "           right_on=[\"right_side_name\"], \n",
    "           how=\"left\")\\\n",
    "    .drop(columns=[\"right_side_name\"])\n",
    "\n",
    "#keep this one if you want to investate the results\n",
    "df_predicted_results_existing_input_3dnode = \\\n",
    "    df_predicted_results[~(df_predicted_results[\"existing_matching_asset\"].isna())]\n",
    "#keep this one if you want to investate the results\n",
    "df_predicted_results_existing_predicted_asset = \\\n",
    "    df_predicted_results[~(df_predicted_results[\"existing_matching_3dnode\"].isna())]\n",
    "\n",
    "df_predicted_results = df_predicted_results[df_predicted_results[\"existing_matching_asset\"].isna()]\n",
    "df_predicted_results = df_predicted_results[df_predicted_results[\"existing_matching_3dnode\"].isna()]\n",
    "\n",
    "# ---\n",
    "# filter based on a list of manual rules\n",
    "\"\"\"\n",
    "rules_from_list = [(\"/[D1]-[L2]-[D3]\", \"[D1]-[L2]-[D3]\")]\n",
    "def get_rule_tuple(row):\n",
    "    return (row[\"inputPattern\"], row[\"predictPattern\"])\n",
    "\n",
    "df_predicted_result = df_predicted_result[df_predicted_result.apply(get_rule_tuple, axis=1)\\\n",
    "    .isin(rules_from_list)]\n",
    "\"\"\"\n",
    "\n",
    "df_predicted_results.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate what rows where thrown away because the input 3d node is already matched\n",
    "df_predicted_results_existing_input_3dnode.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# investigate what rows where thrown away because the asset is already mapped to some 3d node\n",
    "df_predicted_results_existing_predicted_asset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count and print rows with several 3d nodes pointing to the same asset\n",
    "df_predicted_count_assets = df_predicted_results.groupby(\"asset_id\", as_index=False)[\"input\"]\\\n",
    "    .count()\\\n",
    "    .rename(columns={\"input\":\"count_assets\"})\n",
    "\n",
    "df_predicted_results\\\n",
    "    .merge(df_predicted_count_assets[df_predicted_count_assets[\"count_assets\"]>1], on=\"asset_id\", how=\"inner\")\\\n",
    "    .sort_values(\"predicted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop (for now as a solution) the mappings with several 3d nodes to the same asset \n",
    "df_predicted_results_unique = df_predicted_results\\\n",
    "    .merge(df_predicted_count_assets[df_predicted_count_assets[\"count_assets\"]== 1], on=\"asset_id\", how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of dictionaries to create ThreeDAssetMapping\n",
    "resulting_asset_mappings =list(df_predicted_results_unique[[\"node_id\",\"asset_id\"]].T.to_dict().values())\n",
    "print(len(resulting_asset_mappings))\n",
    "resulting_asset_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ThreeDAssetMappings\n",
    "cdf_asset_mappings = []\n",
    "for asset_mapping_dict in resulting_asset_mappings:\n",
    "    cdf_asset_mappings.append(ThreeDAssetMapping(**asset_mapping_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Uncomment to write to clean:\n",
    "#client.three_d.asset_mappings.create(model_id, revision_id, cdf_asset_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
