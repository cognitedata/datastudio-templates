{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cognite.client import CogniteClient\n",
    "\n",
    "from cognite.client.data_classes.three_d import ThreeDAssetMapping\n",
    "\n",
    "from cognite.datastudio.entity_matcher import EntityMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import chunk_create_rules_df, chunk_predict, get_matches_with_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project = \"akerbp\"\n",
    "api_key_name = \"AKERBP_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you need to add api-key\n",
    "#from add_client_api_key import ClientApiKeyWidget\n",
    "#client_api_key_widget = ClientApiKeyWidget(api_key_name=api_key_name, project=project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = CogniteClient(os.environ[api_key_name], project, \"local-jupyter-notebook\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define 3d model_id and revision\n",
    "model_id = 1078941578276888\n",
    "revision_id = 506407845865623\n",
    "\n",
    "# define root_id for assets\n",
    "root_id = 8129784932439587\n",
    "\n",
    "#define a name to store thre predicted result\n",
    "entity_matcher_results_file = \"enma_skarv_fpso.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we provide functions to install 3d-nodes, assets and asset-mappings\n",
    "from data_load_cdf import load_assets, load_threednodes, filter_df_threednodes, load_asset_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download 3d nodes, it might take time\n",
    "df_threednodes = load_threednodes(client, model_id, revision_id)\n",
    "# filter the names of the 3d nodes that do not need contexualization\n",
    "df_threednodes = filter_df_threednodes(df_threednodes, key_words=(\"EQUIPMENT\", \"BRANCH\", \"STRUCTURE\", \" of \"))\n",
    "df_threednodes.rename(columns={\"name\": \"left_side_name\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download assets\n",
    "df_assets = load_assets(client, root_id).rename(columns={\"name\": \"right_side_name\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download existing asset mappings from the 3d model\n",
    "df_asset_mappings = load_asset_mappings(client, model_id, revision_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since df_asset_mappings includes only IDs,\n",
    "# in order to get the names we join on the \n",
    "# df_assets, df_threednodes including available respective IDs and names.\n",
    "df_existing_matches = (\n",
    "        df_asset_mappings[[\"nodeId\", \"assetId\"]]\n",
    "        .merge(\n",
    "            df_assets[[\"id\", \"right_side_name\"]],\n",
    "            how=\"left\",\n",
    "            left_on=\"assetId\",\n",
    "            right_on=\"id\",\n",
    "        )\n",
    "        .drop(columns=\"id\")\n",
    "        .merge(\n",
    "            df_threednodes[[\"id\", \"left_side_name\"]],\n",
    "            how=\"left\",\n",
    "            left_on=\"nodeId\",\n",
    "            right_on=\"id\",\n",
    "        )[[\"left_side_name\", \"right_side_name\"]]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize the entity matcher\n",
    "entity_matcher = EntityMatcher(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rules form the existing mappings if exist\n",
    "df_matches = \\\n",
    "    df_existing_matches[[\"left_side_name\", \"right_side_name\"]]\\\n",
    "    .dropna()\\\n",
    "    .rename(columns = {\"left_side_name\": \"input\", \"right_side_name\": \"predicted\"})\n",
    "df_matches[\"score\"] = 1.0\n",
    "\n",
    "pd_rules_from_existing = chunk_create_rules_df(entity_matcher, df_matches.to_dict('records'), size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions \n",
    "if os.path.exists(entity_matcher_results_file):\n",
    "    print(\"Loading predicted from local...\")\n",
    "    with open(entity_matcher_results_file, \"r\") as f:\n",
    "        predicted_matches = json.load(f)\n",
    "else:\n",
    "    model = entity_matcher.fit(df_assets[\"right_side_name\"].tolist())\n",
    "    predicted_matches = chunk_predict(model, df_threednodes[\"left_side_name\"].tolist(), 100000)\n",
    "    # store all predictions in a file\n",
    "    with open(entity_matcher_results_file, \"w\") as f:\n",
    "        json.dump(predicted_matches, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_matches filter on NAs, \n",
    "# NEEDreset index to match order before creating rules\n",
    "df_predicted_matches = pd.DataFrame.from_dict(predicted_matches).dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create rules for predicted matches\n",
    "pd_rules_from_predicted = chunk_create_rules_df(entity_matcher, df_predicted_matches.to_dict('records'), size=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# associate matches with rules\n",
    "df_predicted_with_rules = get_matches_with_rules(df_predicted_matches, pd_rules_from_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assosicate predicted results with IDs\n",
    "df_predicted_results_raw = df_predicted_with_rules\\\n",
    "    .merge(df_assets, left_on=\"predicted\", right_on=\"right_side_name\", how=\"inner\")\\\n",
    "    .drop(columns=[\"right_side_name\"])\\\n",
    "    .rename(columns={\"id\":\"asset_id\"})\\\n",
    "    .merge(df_threednodes, left_on=\"input\", right_on=\"left_side_name\", how=\"inner\")\\\n",
    "    .drop(columns=[\"left_side_name\"])\\\n",
    "    .rename(columns={\"id\":\"node_id\"})\n",
    "df_predicted_results_raw.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comment or uncomment for different filtering\n",
    "df_predicted_results = df_predicted_results_raw.copy()\n",
    "\n",
    "# filter on the score value \n",
    "df_predicted_result = df_predicted_result[df_predicted_result[\"score\"] > 0.0]\n",
    "\n",
    "# filter on the avgScore value\n",
    "df_predicted_result = df_predicted_result[df_predicted_result[\"avgScore\"] > 0.0]\n",
    "\n",
    "# filter by the number of matcher per rule\n",
    "df_predicted_result = df_predicted_result[df_predicted_result[\"numMatches\"] > 0]\n",
    "\n",
    "# filter by merging on existing rules only\n",
    "df_predicted_result = df_predicted_result.merge(pd_rules_from_existing\\\n",
    "    .rename(columns={\"numMatches\": \"numMatchesExisting\"})\n",
    "    .drop(columns=[\"avgScore\",\"matchIndex\"]), on=[\"inputPattern\", \"predictPattern\"],\n",
    "    how=\"inner\")\n",
    "\n",
    "# filter out input with existing asset mappings\n",
    "df_predicted_result = df_predicted_result\\\n",
    "    .merge(df_existing_matches.rename(columns={\"right_side_name\": \"existing_match\"}), left_on=[\"input\"], right_on=[\"left_side_name\"], how=\"left\")\\\n",
    "    .drop(columns=[\"left_side_name\"])\n",
    "df_predicted_result= df_predicted_result[df_predicted_result[\"existing_match\"].isna()]\n",
    "\n",
    "# filter based on a list of manual rules\n",
    "\"\"\"\n",
    "rules_from_list = [(\"/[D1]-[L2]-[D3]\", \"[D1]-[L2]-[D3]\")]\n",
    "def get_rule_tuple(row):\n",
    "    return (row[\"inputPattern\"], row[\"predictPattern\"])\n",
    "\n",
    "df_predicted_result = df_predicted_result[df_predicted_result.apply(get_rule_tuple, axis=1)\\\n",
    "    .isin(rules_from_list)]\n",
    "\"\"\"\n",
    "\n",
    "df_predicted_result.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of dictionaries to create ThreeDAssetMapping\n",
    "resulting_asset_mappings =list(df_predicted_result[[\"node_id\",\"asset_id\"]].T.to_dict().values())\n",
    "print(len(resulting_asset_mappings))\n",
    "resulting_asset_mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ThreeDAssetMappings\n",
    "cdf_asset_mappings = []\n",
    "for asset_mapping_dict in resulting_asset_mappings:\n",
    "    cdf_asset_mappings.append(ThreeDAssetMapping(**asset_mapping_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to write to clean:\n",
    "#client.three_d.asset_mappings.create(model_id, revision_id, cdf_asset_mappings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
